{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1qrli1NVhMO6BWXMypr6c1axEo2YSp_4U","authorship_tag":"ABX9TyO+nVyPE692NaIlp7vr53qU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"322f2b4a381642e390930c5aee0c47bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9c4b7c2a7f24384907b35ab415b1ffa","IPY_MODEL_6394778a76df43e4af396c2d7c72ead6","IPY_MODEL_fc395aa7e93a40ecae04798f1fac3c9c"],"layout":"IPY_MODEL_c05e92b9eb80424fa21cd03f9c9196df"}},"d9c4b7c2a7f24384907b35ab415b1ffa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_147fa9f2c9e14f388a4ebff90109d4b1","placeholder":"​","style":"IPY_MODEL_31b2457a618441c8a0ee5fbabe5027c9","value":"Loading checkpoint shards: 100%"}},"6394778a76df43e4af396c2d7c72ead6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f163781921dd4fd3bd8ed2bf3d22025f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77ece38cb0d743d381675a37776b5270","value":2}},"fc395aa7e93a40ecae04798f1fac3c9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12b18d1be9f546b08a903e1e8a056e9c","placeholder":"​","style":"IPY_MODEL_24357fdc39ae49d4a32c902078f8e63a","value":" 2/2 [00:40&lt;00:00, 19.47s/it]"}},"c05e92b9eb80424fa21cd03f9c9196df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"147fa9f2c9e14f388a4ebff90109d4b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31b2457a618441c8a0ee5fbabe5027c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f163781921dd4fd3bd8ed2bf3d22025f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77ece38cb0d743d381675a37776b5270":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12b18d1be9f546b08a903e1e8a056e9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24357fdc39ae49d4a32c902078f8e63a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c652f1dbb774af9bb1e2fad18a3671e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8db02d28d96a4165b9045f4285411a51","IPY_MODEL_38b7796d164e4b1e9f8902e0cf1403b0","IPY_MODEL_a715e8e6366f462cba0ff594b9e661c5"],"layout":"IPY_MODEL_55474dd8abe248338a9792147d1e71c0"}},"8db02d28d96a4165b9045f4285411a51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1392e71f231b4742b4df658397f868e2","placeholder":"​","style":"IPY_MODEL_7e98caee786246869e6b0bbef43ec20b","value":"adapter_model.safetensors: 100%"}},"38b7796d164e4b1e9f8902e0cf1403b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e3b4f8d2f6744fc8b80257eeee5ac1a","max":268855232,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db2798999f294e8eb5d1004dcc29246d","value":268855207}},"a715e8e6366f462cba0ff594b9e661c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24110a13ce514437a272fb18b2f9ffb2","placeholder":"​","style":"IPY_MODEL_6ffc66ccdf614363b32a9792bf8f2e62","value":" 269M/269M [00:02&lt;00:00, 252MB/s]"}},"55474dd8abe248338a9792147d1e71c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1392e71f231b4742b4df658397f868e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e98caee786246869e6b0bbef43ec20b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e3b4f8d2f6744fc8b80257eeee5ac1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db2798999f294e8eb5d1004dcc29246d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24110a13ce514437a272fb18b2f9ffb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ffc66ccdf614363b32a9792bf8f2e62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Import libraries\n","import numpy as np\n","import os\n","from PIL import Image\n","import random\n","\n","# Set random seed\n","random.seed(42)\n","\n","# Unload all data such that it's easily accessible\n","main_data_path = \"/content/drive/MyDrive/ENGSCI/4TH YEAR/fall 4th year/ESC499/Code Test/Finetuning/Data/\"\n","training_data_indices = main_data_path+\"INDICES.npz\"\n","training_data_path = main_data_path+\"training\"\n","test_data_path = main_data_path+\"test\""],"metadata":{"id":"twDSWNOuFHSt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 42s 44s\n","%%capture\n","!pip install unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"],"metadata":{"id":"lrHhJrZvFIuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eCDYVlk6swk","executionInfo":{"status":"ok","timestamp":1739066524666,"user_tz":300,"elapsed":32567,"user":{"displayName":"Felicia Liu","userId":"08082401212792009907"}},"outputId":"d74b95d6-e214-43fa-b4d9-9963db2dbe46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.9.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["!pip install --upgrade torch torchvision torchaudio # Upgrading PyTorch to the latest version\n","import torchvision\n","from unsloth import FastVisionModel # FastLanguageModel for LLMs\n","import torch\n","from unsloth import is_bf16_supported\n","from unsloth.trainer import UnslothVisionDataCollator\n","from trl import SFTTrainer, SFTConfig\n","from transformers import TextStreamer\n","from transformers import TrainerCallback, TrainingArguments"]},{"cell_type":"code","source":["# Mount the Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJGjNNso8yvd","executionInfo":{"status":"ok","timestamp":1739066609651,"user_tz":300,"elapsed":619,"user":{"displayName":"Felicia Liu","userId":"08082401212792009907"}},"outputId":"907f88c7-9b8d-469a-f452-dcd628ac1ca7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Functions (TEST VERSION)\n","\n","def load_scan_from_npz(file_path):\n","    data = np.load(file_path)\n","    return data['voxel'], data['ax'], data['sag'], data['cor'], data['label']\n","\n","def create_image_dataset(file_path):\n","    v, a, s, c, l = load_scan_from_npz(file_path)\n","    begin, end = a\n","    image_label_pairs = []\n","    for i in range(begin+20, end+1-20, 1):\n","        np_array, label = v[i], l\n","        np_array = np.uint8(255 * (np_array - np.min(np_array)) / (np.max(np_array) - np.min(np_array)))\n","        image = Image.fromarray(np_array)\n","        image_label_pairs.append({\"image\":image, \"label\":label})\n","    return image_label_pairs\n","\n","def counts_zeros_and_ones(full_list):\n","    zeros = [entry for entry in full_list if entry[\"label\"] == 0]\n","    ones = [entry for entry in full_list if entry[\"label\"] == 1]\n","    num_zeros, num_ones = len(zeros), len(ones)\n","    print(f\"Number of ZEROS: {len(zeros)} || Number of ONES: {len(ones)}\")\n","    return num_zeros, num_ones, zeros, ones\n","\n","def convert_to_conversation(sample):\n","    instruction = \"Classify the brain scan as Low Grade Glioma (0), High Grade Glioma (1), or No Glioma (2). Respond only in the following format: Choice: <0, 1, or 2> Reasoning: <Provide concise reasoning using 10 keywords based on the scan's visual features>.\"\n","\n","    conversation = [\n","        { \"role\": \"user\",\n","          \"content\" : [\n","            {\"type\" : \"text\",  \"text\"  : instruction},\n","            {\"type\" : \"image\", \"image\" : sample['image']} ]\n","        },\n","        { \"role\" : \"assistant\",\n","          \"content\" : [\n","            {\"type\": \"text\", \"text\": f\"Choice: {sample['label']} Reasoning:...\"} ]\n","        },\n","    ]\n","    return { \"messages\" : conversation }\n","\n","def create_conversation_dataset(training_data_path):\n","\n","    # Extract all the patients and the corresponding filenames\n","    filenames = []\n","    for filename in os.listdir(training_data_path):\n","        if filename.endswith(\".npz\"):\n","            file_path = os.path.join(training_data_path, filename)\n","            filenames.append(file_path)\n","    filenames = sorted(filenames, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n","\n","    # Now we build the dataset\n","    patients = []\n","    for filename in filenames:\n","        image_label_pairs = create_image_dataset(filename)\n","        patients.append(image_label_pairs)\n","\n","    # # Now convert the dataset into input for LLM\n","    llm_patients = []\n","    for patient in patients:\n","        llm_patient = [convert_to_conversation(sample) for sample in patient]\n","        llm_patients.append(llm_patient)\n","    print(f\"Number of patients: {len(llm_patients)}\")\n","    print(llm_patients[0][0])\n","\n","    return patients, llm_patients\n","\n","data_patients, llm_patients = create_conversation_dataset(test_data_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tbAA4WQ60Ws","executionInfo":{"status":"ok","timestamp":1739066612500,"user_tz":300,"elapsed":1994,"user":{"displayName":"Felicia Liu","userId":"08082401212792009907"}},"outputId":"4c766f29-badc-4183-d4ce-03659741c70c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of patients: 55\n","{'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': \"Classify the brain scan as Low Grade Glioma (0), High Grade Glioma (1), or No Glioma (2). Respond only in the following format: Choice: <0, 1, or 2> Reasoning: <Provide concise reasoning using 10 keywords based on the scan's visual features>.\"}, {'type': 'image', 'image': <PIL.Image.Image image mode=L size=128x128 at 0x7EA7A9CF4E10>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Choice: 0 Reasoning:...'}]}]}\n"]}]},{"cell_type":"code","source":["# Add the name of the model we want to import\n","# lora_model_name = \"liufelic/newlora_100step_model\"\n","lora_model_name = \"liufelic/newlora_100step_model\"\n","lora_model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n","\n","# Load the model we previously trained\n","from unsloth import FastVisionModel\n","model, tokenizer = FastVisionModel.from_pretrained(\n","    model_name = lora_model_name, # YOUR MODEL YOU USED FOR TRAINING\n","    load_in_4bit = True, # Set to False for 16bit LoRA\n",")\n","FastVisionModel.for_inference(model) # Enable for inference!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["322f2b4a381642e390930c5aee0c47bf","d9c4b7c2a7f24384907b35ab415b1ffa","6394778a76df43e4af396c2d7c72ead6","fc395aa7e93a40ecae04798f1fac3c9c","c05e92b9eb80424fa21cd03f9c9196df","147fa9f2c9e14f388a4ebff90109d4b1","31b2457a618441c8a0ee5fbabe5027c9","f163781921dd4fd3bd8ed2bf3d22025f","77ece38cb0d743d381675a37776b5270","12b18d1be9f546b08a903e1e8a056e9c","24357fdc39ae49d4a32c902078f8e63a","9c652f1dbb774af9bb1e2fad18a3671e","8db02d28d96a4165b9045f4285411a51","38b7796d164e4b1e9f8902e0cf1403b0","a715e8e6366f462cba0ff594b9e661c5","55474dd8abe248338a9792147d1e71c0","1392e71f231b4742b4df658397f868e2","7e98caee786246869e6b0bbef43ec20b","3e3b4f8d2f6744fc8b80257eeee5ac1a","db2798999f294e8eb5d1004dcc29246d","24110a13ce514437a272fb18b2f9ffb2","6ffc66ccdf614363b32a9792bf8f2e62"]},"collapsed":true,"id":"bFuecXor60dq","executionInfo":{"status":"ok","timestamp":1739067199240,"user_tz":300,"elapsed":60466,"user":{"displayName":"Felicia Liu","userId":"08082401212792009907"}},"outputId":"21811a95-9d35-4f77-bd47-21c1375557d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.2.4: Fast Mllama vision patching. Transformers: 4.48.2.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322f2b4a381642e390930c5aee0c47bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["adapter_model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c652f1dbb774af9bb1e2fad18a3671e"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): MllamaForConditionalGeneration(\n","      (vision_model): MllamaVisionModel(\n","        (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n","        (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n","          (tile_embedding): Embedding(9, 8197120)\n","        )\n","        (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n","          (embedding): Embedding(9, 5120)\n","        )\n","        (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n","          (embedding): Embedding(9, 5120)\n","        )\n","        (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (transformer): MllamaVisionEncoder(\n","          (layers): ModuleList(\n","            (0-12): 13 x MllamaVisionEncoderLayer(\n","              (self_attn): MllamaVisionSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaVisionMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=5120, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (fc2): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=5120, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (13): MllamaVisionEncoderLayer(\n","              (self_attn): MllamaVisionSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaVisionMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=5120, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (fc2): lora.Linear(\n","                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=5120, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (14-31): 18 x MllamaVisionEncoderLayer(\n","              (self_attn): MllamaVisionSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaVisionMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=5120, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (fc2): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=5120, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (global_transformer): MllamaVisionEncoder(\n","          (layers): ModuleList(\n","            (0-7): 8 x MllamaVisionEncoderLayer(\n","              (self_attn): MllamaVisionSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaVisionMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1280, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=5120, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (fc2): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=5120, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1280, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","      )\n","      (language_model): MllamaForCausalLM(\n","        (model): MllamaTextModel(\n","          (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n","          (layers): ModuleList(\n","            (0): MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (1): MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (2): MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (3): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (8): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (13): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (18): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (23): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (28): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (33): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (38): MllamaCrossAttentionDecoderLayer(\n","              (cross_attn): MllamaTextCrossSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","            (39): MllamaSelfAttentionDecoderLayer(\n","              (self_attn): MllamaTextSelfSdpaAttention(\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (k_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (o_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","              )\n","              (mlp): MllamaTextMLP(\n","                (gate_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (up_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (down_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Identity()\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (act_fn): SiLU()\n","              )\n","              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","            )\n","          )\n","          (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n","          (rotary_emb): MllamaRotaryEmbedding()\n","        )\n","        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n","      )\n","      (multi_modal_projector): Linear(in_features=7680, out_features=4096, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import re\n","\n","def extract_choice(output_text):\n","    match = re.search(r\"Choice:\\s*[012]\\b\", output_text)\n","    if match: return match.group(0)\n","    else: return output_text.replace(\"\\n\", \" ||| \")\n","\n","def write_file_for_one_patient(patient, path):\n","    # Open the file in append mode (if it doesn't exist, it will be created)\n","    with open(path, 'w') as file:\n","        for sample_id in range(len(patient)):\n","\n","            # Extract the info from the message\n","            sample = patient[sample_id]\n","            image = sample[\"image\"]\n","            ground_truth = sample[\"label\"]\n","\n","            # Instruction without reasoning\n","            instruction = \"Classify the brain scan as Low Grade Glioma (0), High Grade Glioma (1), or No Glioma (2) based on the scan's visual features. Respond only in the following format: Choice: <0, 1, or 2>.\"\n","            messages = [\n","                {\"role\": \"user\", \"content\": [\n","                    {\"type\": \"image\"},\n","                    {\"type\": \"text\", \"text\": instruction}\n","                ]}\n","            ]\n","            input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n","            inputs = tokenizer(\n","                image,\n","                input_text,\n","                add_special_tokens=False,\n","                return_tensors=\"pt\",\n","            ).to(\"cuda\")\n","\n","            # Generate the tokens\n","            output_tokens = model.generate(**inputs, max_new_tokens=20,\n","                                           use_cache=True, temperature=1.5, min_p=0.1)\n","            generated_text = extract_choice(tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n","\n","            # Prepare the output message\n","            result_message = f\"Ground truth: {ground_truth} || Model output: {generated_text}\"\n","\n","            # Print to the terminal\n","            # print(result_message)\n","\n","            # Write to the file\n","            file.write(result_message + '\\n')  # Add a newline after each entry\n","\n","    return\n"],"metadata":{"id":"9tYBqHvq60gH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unload all data such that it's easily accessible\n","main_results_folder = \"/content/drive/MyDrive/ENGSCI/4TH YEAR/fall 4th year/ESC499/Code Test/Finetuning/Results/\"\n","\n","# for patient_id in range(len(data_patients)):\n","for patient_id in range(31, 55, 1):\n","\n","    print(f\"Patient ID: {patient_id}\")\n","    patient = data_patients[patient_id]\n","\n","    # path for patient data storage\n","    patient_results_text = main_results_folder + f\"patient_{patient_id}.txt\"\n","    write_file_for_one_patient(patient, patient_results_text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoICtA8u60iW","executionInfo":{"status":"ok","timestamp":1739072018605,"user_tz":300,"elapsed":2003501,"user":{"displayName":"Felicia Liu","userId":"08082401212792009907"}},"outputId":"06549d56-cac5-425a-b541-455fb118ee11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Patient ID: 31\n","Patient ID: 32\n","Patient ID: 33\n","Patient ID: 34\n","Patient ID: 35\n","Patient ID: 36\n","Patient ID: 37\n","Patient ID: 38\n","Patient ID: 39\n","Patient ID: 40\n","Patient ID: 41\n","Patient ID: 42\n","Patient ID: 43\n","Patient ID: 44\n","Patient ID: 45\n","Patient ID: 46\n","Patient ID: 47\n","Patient ID: 48\n","Patient ID: 49\n","Patient ID: 50\n","Patient ID: 51\n","Patient ID: 52\n","Patient ID: 53\n","Patient ID: 54\n"]}]}]}